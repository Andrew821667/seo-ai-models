#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SEO –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π seo-ai-models.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ GitHub Actions –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ legalaipro.ru.
"""

import argparse
import json
import sys
import os
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –ø–æ–∏—Å–∫–∞ –º–æ–¥—É–ª–µ–π
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

try:
    from seo_ai_models.parsers.spa_parser import SPAParser
    from seo_ai_models.models.seo_advisor.analyzers.enhanced_content_analyzer import EnhancedContentAnalyzer
    MODULES_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑: pip install -r requirements.txt")
    MODULES_AVAILABLE = False


def analyze_url_full(url):
    """
    –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ URL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SPAParser –∏ EnhancedContentAnalyzer.
    
    Args:
        url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        dict: –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"\n{'='*60}")
    print(f"üîç SEO –ê–ù–ê–õ–ò–ó: {url}")
    print(f"{'='*60}\n")
    
    if not MODULES_AVAILABLE:
        print("‚ùå –ú–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑.")
        return None
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print("‚öôÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SPAParser...")
        parser = SPAParser(
            wait_for_load=7000,
            wait_for_timeout=45000,
            record_ajax=True
        )
        
        # –ü–∞—Ä—Å–∏–º URL
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}...")
        parsed_data = parser.parse(url)
        
        if not parsed_data.get("success"):
            error_msg = parsed_data.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {error_msg}")
            return {
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": error_msg
            }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        print("üìù –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        content = ""
        if "content" in parsed_data and "all_text" in parsed_data["content"].get("content", {}):
            content = parsed_data["content"]["content"]["all_text"]
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            for paragraph in parsed_data.get("content", {}).get("content", {}).get("paragraphs", []):
                content += paragraph + "\n\n"
        
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        html_content = parsed_data.get("html", "")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        print("‚öôÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EnhancedContentAnalyzer...")
        analyzer = EnhancedContentAnalyzer()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
        print("üî¨ –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        metrics = analyzer.analyze_content(content, html_content)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        keywords = [
            "—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —É—Å–ª—É–≥–∏",
            "—é—Ä–∏—Å—Ç",
            "–ø—Ä–∞–≤–æ–≤–∞—è –ø–æ–º–æ—â—å",
            "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è",
            "–¥–æ–≥–æ–≤–æ—Ä"
        ]
        print("üîë –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
        keyword_analysis = analyzer.extract_keywords(content, keywords)
        
        print("\n‚ú® –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!\n")
        
        return {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "status": "success",
            "parsed_data": parsed_data,
            "content_length": len(content),
            "metrics": metrics,
            "keyword_analysis": keyword_analysis,
            "html_length": len(html_content)
        }
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }


def generate_recommendations(analysis):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
    """
    recommendations = []
    
    recommendations.append("# üéØ SEO –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è legalaipro.ru\n\n")
    recommendations.append(f"**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\n")
    recommendations.append(f"**URL:** {analysis.get('url', 'N/A')}\n")
    recommendations.append(f"**–°—Ç–∞—Ç—É—Å:** {analysis.get('status', 'N/A')}\n\n")
    
    if analysis.get('status') == 'success':
        recommendations.append("## ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n\n")
        
        content_len = analysis.get('content_length', 0)
        recommendations.append(f"- **–û–±—ä—ë–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞:** {content_len} —Å–∏–º–≤–æ–ª–æ–≤\n")
        
        if content_len < 1000:
            recommendations.append("  ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ! –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –º–∏–Ω–∏–º—É–º: 2000 —Å–∏–º–≤–æ–ª–æ–≤\n")
        elif content_len < 2000:
            recommendations.append("  ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç–∞ –º–∞–ª–æ–≤–∞—Ç–æ. –î–æ–±–∞–≤—å—Ç–µ –µ—â—ë —Ö–æ—Ç—è –±—ã 1000 —Å–∏–º–≤–æ–ª–æ–≤\n")
        else:
            recommendations.append("  ‚úÖ –û–±—ä—ë–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π\n")
        
        recommendations.append("\n")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = analysis.get('metrics', {})
        if metrics:
            recommendations.append("### üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n\n")
            for key, value in metrics.items():
                recommendations.append(f"- **{key}**: {value}\n")
            recommendations.append("\n")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        kw_analysis = analysis.get('keyword_analysis', {})
        if kw_analysis:
            recommendations.append("### üîë –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n\n")
            for kw, data in kw_analysis.items():
                recommendations.append(f"- **{kw}**: {data}\n")
            recommendations.append("\n")
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞
    recommendations.append("## üí° –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n")
    
    recommendations.append("### üìù –ö–æ–Ω—Ç–µ–Ω—Ç\n")
    recommendations.append("- –î–æ–±–∞–≤—å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –ø—Ä–∞–≤–æ–≤—ã–º –≤–æ–ø—Ä–æ—Å–∞–º (–º–∏–Ω–∏–º—É–º 2000 –∑–Ω–∞–∫–æ–≤)\n")
    recommendations.append("- –°–æ–∑–¥–∞–π—Ç–µ FAQ —Ä–∞–∑–¥–µ–ª —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–æ–≤\n")
    recommendations.append("- –ü—É–±–ª–∏–∫—É–π—Ç–µ –∫–µ–π—Å—ã —É—Å–ø–µ—à–Ω—ã—Ö –¥–µ–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ä–µ—à–µ–Ω–∏—è\n")
    recommendations.append("- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ\n\n")
    
    recommendations.append("### üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ SEO\n")
    recommendations.append("- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ meta-—Ç–µ–≥–∏ (title, description) –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n")
    recommendations.append("- –î–æ–±–∞–≤—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ Schema.org: Organization, LegalService, Attorney\n")
    recommendations.append("- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ (—Ü–µ–ª—å: < 3 —Å–µ–∫):\n")
    recommendations.append("  - –°–∂–∞—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (WebP —Ñ–æ—Ä–º–∞—Ç)\n")
    recommendations.append("  - –ú–∏–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è CSS/JS\n")
    recommendations.append("  - –í–∫–ª—é—á–µ–Ω–∏–µ gzip/brotli —Å–∂–∞—Ç–∏—è\n")
    recommendations.append("- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π XML sitemap –∏ robots.txt\n")
    recommendations.append("- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ rel='canonical' –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–µ–π\n\n")
    
    recommendations.append("### üìç –õ–æ–∫–∞–ª—å–Ω–æ–µ SEO\n")
    recommendations.append("- –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –≤ –Ø–Ω–¥–µ–∫—Å.–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ –∏ Google My Business\n")
    recommendations.append("- –î–æ–±–∞–≤—å—Ç–µ –∞–¥—Ä–µ—Å –æ—Ñ–∏—Å–∞ –∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã –Ω–∞ –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n")
    recommendations.append("- –í—Å—Ç—Ä–æ–π—Ç–µ Google Maps —Å –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º –æ—Ñ–∏—Å–∞\n")
    recommendations.append("- –£–∫–∞–∂–∏—Ç–µ —á–∞—Å—ã —Ä–∞–±–æ—Ç—ã –∏ —Å–ø–æ—Å–æ–±—ã —Å–≤—è–∑–∏\n\n")
    
    recommendations.append("### üéØ –ö–æ–Ω–≤–µ—Ä—Å–∏—è\n")
    recommendations.append("- –†–∞–∑–º–µ—Å—Ç–∏—Ç–µ —á—ë—Ç–∫–∏–µ –ø—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é (CTA)\n")
    recommendations.append("- –î–æ–±–∞–≤—å—Ç–µ –æ–Ω–ª–∞–π–Ω-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∏–ª–∏ —á–∞—Ç-–±–æ—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å–≤—è–∑–∏\n")
    recommendations.append("- –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ (–º–∞–∫—Å. 3-5 –ø–æ–ª–µ–π)\n")
    recommendations.append("- –î–æ–±–∞–≤—å—Ç–µ —Ä–∞–∑–¥–µ–ª —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\n")
    recommendations.append("- –£–∫–∞–∂–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã (—Ü–∏—Ñ—Ä—ã, –∫–µ–π—Å—ã)\n\n")
    
    recommendations.append("---\n")
    recommendations.append(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–∏—Å—Ç–µ–º–æ–π SEO –∞–Ω–∞–ª–∏–∑–∞ seo-ai-models v1.0*\n")
    recommendations.append(f"*–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}*\n")
    
    return "".join(recommendations)


def main():
    parser = argparse.ArgumentParser(
        description='–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SEO –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ –¥–ª—è GitHub Actions'
    )
    parser.add_argument(
        '--url',
        type=str,
        required=True,
        help='URL —Å–∞–π—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
    )
    
    args = parser.parse_args()
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    results = analyze_url_full(args.url)
    
    if results:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        output_file = "analysis_result.json"
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}...")
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                results,
                f,
                ensure_ascii=False,
                indent=2,
                default=lambda x: x.isoformat() if isinstance(x, datetime) else str(x)
            )
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = generate_recommendations(results)
        rec_file = "recommendations_ru.md"
        
        print(f"\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ {rec_file}...")
        with open(rec_file, "w", encoding="utf-8") as f:
            f.write(recommendations)
        
        print(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {rec_file}")
        
        print(f"\n{'='*60}")
        print("üéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù –£–°–ü–ï–®–ù–û!")
        print(f"{'='*60}\n")
        
        return 0 if results.get('status') == 'success' else 1
    else:
        print("\n‚ùå –ù–ï –£–î–ê–õ–û–°–¨ –í–´–ü–û–õ–ù–ò–¢–¨ –ê–ù–ê–õ–ò–ó")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n")
        return 1


if __name__ == "__main__":
    sys.exit(main())
