#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SEO –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π seo-ai-models.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ GitHub Actions –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ legalaipro.ru.
"""

import argparse
import json
import sys
import os
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –ø–æ–∏—Å–∫–∞ –º–æ–¥—É–ª–µ–π
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

try:
    from seo_ai_models.parsers.unified.unified_parser import UnifiedParser
    from seo_ai_models.models.seo_advisor.analyzers.enhanced_content_analyzer import EnhancedContentAnalyzer
    MODULES_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑: pip install -r requirements.txt")
    MODULES_AVAILABLE = False

def analyze_url_full(url):
    """
    –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ URL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º UnifiedParser –∏ EnhancedContentAnalyzer.
    Args:
        url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        dict: –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"\n{'='*60}")
    print(f"üîç SEO –ê–ù–ê–õ–ò–ó: {url}")
    print(f"{'='*60}\n")

    if not MODULES_AVAILABLE:
        print("‚ùå –ú–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑.")
        return None

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    print("‚öôÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedParser...")
    parser = UnifiedParser(
        force_spa_mode=True,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º SPA-—Ä–µ–∂–∏–º –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        auto_detect_spa=True,  # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ force_spa_mode –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç
    )

    # –ü–∞—Ä—Å–∏–º URL
    print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}...")
    try:
        parsed_data = parser.parse_url(url)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
        import traceback
        traceback.print_exc()
        return None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
    if not parsed_data:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É (–ø–∞—Ä—Å–µ—Ä –≤–µ—Ä–Ω—É–ª None)")
        return None

    print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞")
    print(f"   –ö–ª—é—á–∏ –≤ parsed_data: {list(parsed_data.keys())}")

    page_data = parsed_data.get('page_data', {})
    metadata = page_data.get('metadata', {})
    print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata.get('title', 'N/A')}")
    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {metadata.get('description', 'N/A')[:100] if metadata.get('description') else 'N/A'}...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    print("\nüìä –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    analyzer = EnhancedContentAnalyzer()

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ HTML –∏–∑ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    text_content = parsed_data.get('text', '') or parsed_data.get('content', '') or ''
    html_content = parsed_data.get('html', '') or parsed_data.get('html_content', '') or ''

    print(f"   –î–ª–∏–Ω–∞ text_content: {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –î–ª–∏–Ω–∞ html_content: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")

    if len(text_content) == 0 and len(html_content) == 0:
        print("   ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –ø–∞—Ä—Å–µ—Ä–∞!")
        print("   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   - –°–∞–π—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç JavaScript –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞")
        print("   - –°–∞–π—Ç –±–ª–æ–∫–∏—Ä—É–µ—Ç –±–æ—Ç–æ–≤")
        print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å Playwright")

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    analysis_result = analyzer.analyze_content(text_content, html_content)

    if not analysis_result:
        print("‚ùå –û—à–∏–±–∫–∞: –ê–Ω–∞–ª–∏–∑ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return None

    print("‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω")

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    analysis_result['metadata'] = {
        'url': url,
        'analyzed_at': datetime.now().isoformat(),
        'analyzer_version': '1.0.0'
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª
    output_file = "analysis_result.json"
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return None

    print(f"\n{'='*60}")
    print("‚úÖ SEO –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û")
    print(f"{'='*60}\n")

    return analysis_result

def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    """
    parser = argparse.ArgumentParser(
        description='–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SEO –∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º seo-ai-models'
    )
    parser.add_argument(
        '--url',
        required=True,
        help='URL —Å–∞–π—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
    )

    args = parser.parse_args()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    result = analyze_url_full(args.url)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞
    if result:
        sys.exit(0)  # –£—Å–ø–µ—Ö
    else:
        sys.exit(1)  # –û—à–∏–±–∫–∞

if __name__ == "__main__":
    main()
